{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13c044db-f13a-4ef1-8d35-12c1b2fdcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import read_trace_code\n",
    "from importlib import reload\n",
    "import tools_on_B_graph as Btools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c623a-c360-49d5-98e1-ae95177f3a53",
   "metadata": {},
   "source": [
    "# Test on mymod :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ce50ca-6a8a-4f37-adc9-cd305b6e99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubMod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential (\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,512),\n",
    "        )\n",
    "    \n",
    "    def forward(self,p1,p2,p3):\n",
    "        y1 = self.linear_stack(p1)\n",
    "        y2 = self.linear_stack(p2)\n",
    "        y3 = self.linear_stack(p3)\n",
    "        return y1+y2,y1-y2+y3,y1\n",
    "\n",
    "class MyMod(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.op = SubMod()\n",
    "    def forward(self,x):\n",
    "        (z1,z2,z3) = self.op(x,-x,p3=torch.ones_like(x))\n",
    "        return z1 + 2*z3 + z2\n",
    "\n",
    "mymod = MyMod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a1263a-3fba-4547-9c6f-018d227fbc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== MYMOD =====\n",
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  op = self.op\n",
      "  input = torch.neg(x)\n",
      "  input0 = torch.ones_like(x, dtype=6, layout=0, device=torch.device(\"cpu\"), pin_memory=False)\n",
      "  _0, _1, _2, = (op).forward(x, input, input0, )\n",
      "  _3 = torch.add(_1, torch.mul(_0, CONSTANTS.c0))\n",
      "  return torch.add(_3, _2)\n",
      "\n",
      "===== SUB CODE : self.op.forward =====\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    input: Tensor,\n",
      "    input0: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
      "  linear_stack = self.linear_stack\n",
      "  _0 = (linear_stack).forward(x, )\n",
      "  _1 = (linear_stack).forward1(input, )\n",
      "  _2 = (linear_stack).forward2(input0, )\n",
      "  z1 = torch.add(_0, _1)\n",
      "  z2 = torch.add(torch.sub(_0, _1), _2)\n",
      "  return (_0, z1, z2)\n",
      "\n",
      "===== SUB SUB CODE =====\n",
      "def forward1(self,\n",
      "    input: Tensor) -> Tensor:\n",
      "  _1 = getattr(self, \"1\")\n",
      "  _0 = getattr(self, \"0\")\n",
      "  _2 = (_1).forward1((_0).forward1(input, ), )\n",
      "  return _2\n",
      "\n",
      "===== SUB SUB SUB CODE =====\n",
      "== to debug : getattr(self.op.linear_stack,\"1\").forward2 ==\n",
      "def forward2(self,\n",
      "    argument_1: Tensor) -> Tensor:\n",
      "  bias = self.bias\n",
      "  weight = self.weight\n",
      "  y3 = torch.linear(argument_1, weight, bias)\n",
      "  return y3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_mymod = torch.rand((128))\n",
    "script_mymod = torch.jit.trace_module(mymod, {'forward': (src_mymod,)},check_trace=False)\n",
    "print(\"===== MYMOD =====\")\n",
    "print(script_mymod.forward.code)\n",
    "\n",
    "print(\"===== SUB CODE : self.op.forward =====\")\n",
    "print(script_mymod.op.code)\n",
    "\n",
    "print(\"===== SUB SUB CODE =====\")\n",
    "print(script_mymod.op.linear_stack.forward1.code)\n",
    "\n",
    "print(\"===== SUB SUB SUB CODE =====\")\n",
    "print(\"== to debug : getattr(self.op.linear_stack,\\\"1\\\").forward2 ==\")\n",
    "print(getattr(script_mymod.op.linear_stack,\"1\").forward2.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10c08759-bec7-4227-b6de-6375fa19dffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening : self.forward\n",
      "Opening : self.op.forward\n",
      "Opening : self.op.linear_stack.forward\n",
      "Opening : getattr(self.op.linear_stack,\"0\").forward\n",
      "Opening : getattr(self.op.linear_stack,\"1\").forward\n",
      "Opening : self.op.linear_stack.forward1\n",
      "Opening : getattr(self.op.linear_stack,\"0\").forward1\n",
      "Opening : getattr(self.op.linear_stack,\"1\").forward1\n",
      "Opening : self.op.linear_stack.forward2\n",
      "Opening : getattr(self.op.linear_stack,\"0\").forward2\n",
      "Opening : getattr(self.op.linear_stack,\"1\").forward2\n",
      "def main(x):\n",
      "\t__10_fv = torch.relu(x)\n",
      "\t__13_y1 = torch.linear(__10_fv,getattr(self.op.linear_stack,\"1\").weight,getattr(self.op.linear_stack,\"1\").bias)\n",
      "\t__2_input = torch.neg(x)\n",
      "\t__18_fv = torch.relu(__2_input)\n",
      "\t__21_y2 = torch.linear(__18_fv,getattr(self.op.linear_stack,\"1\").weight,getattr(self.op.linear_stack,\"1\").bias)\n",
      "\t__30_z1 = torch.add(__13_y1,__21_y2)\n",
      "\t__32_fv = torch.sub(__13_y1,__21_y2)\n",
      "\t__4_fv = torch.device(cpu)\n",
      "\t__3_input0 = torch.ones_like(x,dtype = 6,layout = 0,device = __4_fv,pin_memory = False)\n",
      "\t__26_fv = torch.relu(__3_input0)\n",
      "\t__29_y3 = torch.linear(__26_fv,getattr(self.op.linear_stack,\"1\").weight,getattr(self.op.linear_stack,\"1\").bias)\n",
      "\t__31_z2 = torch.add(__32_fv,__29_y3)\n",
      "\t__33_fv = (__13_y1,__30_z1,__31_z2)\n",
      "\t__35__1 = getattr(__33_fv,\"1\")\n",
      "\t__34__0 = getattr(__33_fv,\"0\")\n",
      "\t__38_fv = torch.mul(__34__0,tensor(2))\n",
      "\t__37__3 = torch.add(__35__1,__38_fv)\n",
      "\t__36__2 = getattr(__33_fv,\"2\")\n",
      "\t__39_fv = torch.add(__37__3,__36__2)\n",
      "\treturn __39_fv\n"
     ]
    }
   ],
   "source": [
    "reload(read_trace_code)\n",
    "reload(Btools)\n",
    "mymod_Bg = read_trace_code.main(mymod,(src_mymod,))\n",
    "Btools.print_code(mymod_Bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93a9ed28-c011-4da9-8284-176b1661599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' strange behavior : \\nclass B_test():\\n    def __init__(self,req = []):\\n        self.re = req\\n        \\nl = B_test()\\nl2 = B_test()\\nl.re.append(5)\\n\\nprint(l.re)\\nprint(l2.re)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" strange behavior : \n",
    "class B_test():\n",
    "    def __init__(self,req = []):\n",
    "        self.re = req\n",
    "        \n",
    "l = B_test()\n",
    "l2 = B_test()\n",
    "l.re.append(5)\n",
    "\n",
    "print(l.re)\n",
    "print(l2.re)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
