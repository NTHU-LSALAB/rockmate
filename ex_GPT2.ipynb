{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8818dbb8-53d3-4417-94b4-fec607741ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import torch.nn as nn\n",
    "import pytorch_checkmate\n",
    "from importlib import reload\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb45e7c-1ab2-4735-bf00-533fbed48d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n",
    "        x = x.view(new_shape)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    \n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        outputs = torch.matmul(scores, v)\n",
    "        return outputs\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(new_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        # out      = self._attn(q, k, v)\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        # if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        out = torch.matmul(scores, v)\n",
    "        \n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=d_model, n_head=n_head, d_head=64, n_ctx=1024, bias=True, scale=False, dropout=dropout)\n",
    "        self.feedforward = FeedForward(dropout=dropout, d_model=d_model, nx=d_model*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln_1(x)\n",
    "        x2 = self.ln_2(x)\n",
    "        x = x + self.attn(x1)\n",
    "        x = x + self.feedforward(x2)\n",
    "        return x\n",
    "    \n",
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257, dropout=0.1):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=d_model, n_head=12, dropout=dropout)\n",
    "        self.h       = _get_clones(block, nlayers)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(dropout)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, src, labels=None, pos_ids=None, return_inp=False, dropout=0.1):\n",
    "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1), device=device).unsqueeze(0)\n",
    "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
    "        if return_inp: return inp \n",
    "        for i in range(self.nlayers): inp = self.h[0](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return outputs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5ba95c-f90b-42f9-9df5-c7128eebd080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 464, 5440, 4534]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context1  = torch.tensor([tokenizer.encode(\"The planet earth\")], device=device)\n",
    "context2  = torch.tensor([tokenizer.encode(\"I'm upset with those tools\")], device=device)\n",
    "print(context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af07f6f-abc0-4e20-8143-f92ffa5477b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (h): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (c_proj): Conv1D()\n",
       "      )\n",
       "      (feedforward): FeedForward(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (wte): Embedding(50257, 384)\n",
       "  (wpe): Embedding(1024, 384)\n",
       "  (drop): Dropout(p=0, inplace=False)\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (out): Linear(in_features=384, out_features=50257, bias=False)\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = GPT2(nlayers=1,dropout=0, d_model=768//2)\n",
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289c36f5-cde5-41ac-92a7-34a9bfc3fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25956/1697464919.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n"
     ]
    }
   ],
   "source": [
    "GPT2_Bg = pytorch_checkmate.read_trace_code.make_B_graph(model2,(context1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae09c493-5848-4ed9-a5b5-d4f3306e284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_checkmate.Dtools.print_all_fw_nodes(GPT2_Bg,print_ast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7fa401-6862-473a-a2f4-047cf2c2eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_Dg = pytorch_checkmate.Dtools.B_to_D(GPT2_Bg,model2,{\"src\":context1})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b83beb7f-5973-4890-83d2-692c7e1948d2",
   "metadata": {},
   "source": [
    "pytorch_checkmate.Dtools.print_D_graph(GPT2_Dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081b4ef3-3dd2-4479-ac6c-999ee6116d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_Sg = pytorch_checkmate.Stools.D_to_S(GPT2_Dg)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddb110ea-8723-4928-ad2e-07ce1ade0cf6",
   "metadata": {},
   "source": [
    "pytorch_checkmate.Stools.print_S_graph(GPT2_Sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9766acc-2b55-4b85-b6f5-f5e784901709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shawn/anaconda3/lib/python3.9/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GPT2_Kg = pytorch_checkmate.Ktools.S_to_K(GPT2_Sg,model2,{\"src\":context1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ec8999-b52f-4afa-b091-0fac344b423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "pytorch_checkmate.Ktools.print_K_graph(GPT2_Kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb925e5-f3d0-431f-8335-50f6f24564c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cost: 827392 max cost: 603136 budget: 700000\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-03-23\n",
      "feasible schedule solved\n"
     ]
    }
   ],
   "source": [
    "budget = 700000\n",
    "reload(pytorch_checkmate)\n",
    "sched_result, g = pytorch_checkmate.use_chk.make_sched(GPT2_Kg, budget, use_gurobi=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fd040d-8684-4ac2-bc65-b38811d3601c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_mem in the solution: 696832\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('max_mem in the solution:', np.max(sched_result.ilp_aux_data.U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61c8b32-cf8a-43e6-9d58-1340ceb4cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = pytorch_checkmate.use_chk.Sched_to_Code(g, GPT2_Kg)\n",
    "\n",
    "sched_code = translator.generate_sched_code(sched_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49323a93-b6c7-49f8-ac87-8fa962f87899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======EXEC SCHED CODE=======\n",
      "weight grad: tensor(0.5005, device='cuda:0')\n",
      "peak memory: 161705984\n"
     ]
    }
   ],
   "source": [
    "## To valid the result is correct\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "mymod = GPT2(nlayers=1,dropout=0, d_model=768//2).to('cuda')\n",
    "context1  = torch.tensor([tokenizer.encode(\"The planet earth\")]).to('cuda')\n",
    "\n",
    "loss = torch.ones_like(mymod(context1),device=device)\n",
    "tmp_local = {'self':mymod, 'src':context1, '_loss':loss}\n",
    "\n",
    "\n",
    "#### Bugs in the code\n",
    "# indices is int type, and should not require gradient\n",
    "sched_code[1] = \"___11__2 = torch.arange(0, __10__1, 1, dtype=torch.int32, device=torch.device('cuda'), pin_memory=False) ; __11__2 = ___11__2.detach()\\n__13_input = torch.unsqueeze(__11__2, 0)\"\n",
    "\n",
    "# replace tensor to torch.tensor\n",
    "sched_code[11] = sched_code[11].replace('tensor', 'torch.tensor')\n",
    "####\n",
    "\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "max_before = torch.cuda.max_memory_allocated()\n",
    "\n",
    "exec(GPT2_Sg.init_node.get_code(), globals(), tmp_local)\n",
    "for i,code in enumerate(sched_code[:]):\n",
    "    #print(i,)\n",
    "    exec(code, globals(), tmp_local)\n",
    "    \n",
    "print(\"=======EXEC SCHED CODE=======\")\n",
    "print(\"weight grad:\", torch.mean(tmp_local['self'].ln_f.weight.grad))\n",
    "print(\"peak memory:\", torch.cuda.max_memory_allocated()-max_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba550227-34c1-4dd9-94b4-cfbc7df0d440",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======ORIGINAL MODULE=======\n",
      "weight grad: tensor(0.5005, device='cuda:0')\n",
      "peak memory: 68920832\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0)\n",
    "model3 = GPT2(nlayers=1,dropout=0, d_model=768//2).to('cuda')\n",
    "context1  = torch.tensor([tokenizer.encode(\"The planet earth\")]).to('cuda')\n",
    "\n",
    "loss = torch.ones_like(model3(context1),device=device)\n",
    "\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "max_before = torch.cuda.max_memory_allocated()\n",
    "\n",
    "y = model3(context1)\n",
    "y.backward(loss)\n",
    "\n",
    "\n",
    "print(\"=======ORIGINAL MODULE=======\")\n",
    "print(\"weight grad:\", torch.mean(model3.ln_f.weight.grad))\n",
    "print(\"peak memory:\", torch.cuda.max_memory_allocated()-max_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8ca38-2eda-4bef-9b65-a41866631937",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice that model is very big in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd988afb-002c-425b-a69a-e3970c515024",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## To understand the memory cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c154360-08a2-4bf6-91c2-281fa952fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code 0 0 0\n",
      "code 1 512 512\n",
      "code 2 512 512\n",
      "code 3 5120 5120\n",
      "code 4 5120 5120\n",
      "code 5 9728 9728\n",
      "code 6 9728 9728\n",
      "code 7 14336 14336\n",
      "code 8 14336 14336\n",
      "code 9 19968 19968\n",
      "code 10 19968 19968\n",
      "code 11 33792 33792\n",
      "code 12 33792 33792\n",
      "code 13 34304 34304\n",
      "code 14 34304 34304\n",
      "code 15 34816 34816\n",
      "code 16 34816 34816\n",
      "code 17 40448 40448\n",
      "code 18 40448 40448\n",
      "code 19 40448 40448\n",
      "code 20 40448 40448\n",
      "code 21 58880 58880\n",
      "code 22 58880 58880\n",
      "code 23 63488 63488\n",
      "code 24 63488 63488\n",
      "code 25 81920 81920\n",
      "code 26 81920 81920\n",
      "code 27 86528 86528\n",
      "code 28 86528 86528\n",
      "code 29 91136 91136\n",
      "code 30 91136 91136\n",
      "code 31 95744 95744\n",
      "code 32 95744 95744\n",
      "code 33 95744 95744\n",
      "code 34 91136 95744\n",
      "code 35 91136 95744\n",
      "code 36 101376 101376\n",
      "code 37 101376 101376\n",
      "code 38 704512 704512\n",
      "code 39 704512 704512\n",
      "code 40 704512 704512\n",
      "code 41 101376 704512\n",
      "code 42 101376 704512\n",
      "code 43 77700608 78906880\n",
      "code 44 77696000 78906880\n",
      "code 45 77696000 78906880\n",
      "code 46 77707264 78906880\n",
      "code 47 77702656 78906880\n",
      "code 48 77698048 78906880\n",
      "code 49 77698048 78906880\n",
      "code 50 78294016 78906880\n",
      "code 51 78289408 78906880\n",
      "code 52 78289408 78906880\n",
      "code 53 78294016 78906880\n",
      "code 54 78289408 78906880\n",
      "code 55 78284800 78906880\n",
      "code 56 78284800 78906880\n",
      "code 57 78299136 78906880\n",
      "code 58 78299136 78906880\n",
      "code 59 78294528 78906880\n",
      "code 60 78294528 78906880\n",
      "code 61 78295040 78906880\n",
      "code 62 78295040 78906880\n",
      "code 63 78295040 78906880\n",
      "code 64 78295040 78906880\n",
      "code 65 78295040 78906880\n",
      "code 66 80674304 80683520\n",
      "code 67 80674304 80683520\n",
      "code 68 80655872 80683520\n",
      "code 69 80651264 80683520\n",
      "code 70 80651264 80683520\n",
      "code 71 80651776 80683520\n",
      "code 72 80651264 80683520\n",
      "code 73 80650752 80683520\n",
      "code 74 80650752 80683520\n",
      "code 75 80669184 80706048\n",
      "code 76 80650752 80706048\n",
      "code 77 80632320 80706048\n",
      "code 78 80632320 80706048\n",
      "code 79 80632320 80706048\n",
      "code 80 80631808 80706048\n",
      "code 81 80617984 80706048\n",
      "code 82 80617984 80706048\n",
      "code 83 80622592 80706048\n",
      "code 84 80622592 80706048\n",
      "code 85 82992640 83029504\n",
      "code 86 82992640 83029504\n",
      "code 87 84771328 84798976\n",
      "code 88 84771328 84798976\n",
      "code 89 84773376 84798976\n",
      "code 90 84768768 84798976\n",
      "code 91 84768256 84798976\n",
      "code 92 84763648 84798976\n",
      "code 93 84749312 84798976\n",
      "code 94 84730880 84798976\n",
      "code 95 84730880 84798976\n",
      "code 96 84730880 84798976\n",
      "code 97 84732928 84798976\n",
      "code 98 84728320 84798976\n",
      "code 99 84125184 84798976\n",
      "code 100 84120576 84798976\n",
      "code 101 84115968 84798976\n",
      "code 102 84115968 84798976\n",
      "code 103 84125184 84798976\n",
      "code 104 84120576 84798976\n",
      "code 105 84115968 84798976\n",
      "code 106 84102144 84798976\n",
      "code 107 84102144 84798976\n",
      "code 108 84102144 84798976\n",
      "code 109 84102144 84798976\n",
      "code 110 84102144 161705984\n",
      "code 111 84102144 161705984\n",
      "code 112 84102144 161705984\n",
      "code 113 85675008 161705984\n",
      "code 114 85674496 161705984\n",
      "code 115 85665280 161705984\n",
      "code 116 85665280 161705984\n"
     ]
    }
   ],
   "source": [
    "from checkmate.core.schedule import OperatorEvaluation, DeallocateRegister, AllocateRegister\n",
    "mem_timeline = []\n",
    "for i,op in enumerate(sched_result.schedule):\n",
    "    if isinstance(op, AllocateRegister):\n",
    "        continue\n",
    "    else:\n",
    "        mem_timeline.append(sched_result.schedule_aux_data.mem_timeline[i])\n",
    "        # mem_timeline.append(sb.ram_timeline[i])\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "mem_timeline_real = []\n",
    "torch.random.manual_seed(0)\n",
    "mymod = GPT2(nlayers=1,dropout=0, d_model=768//2).to('cuda')\n",
    "context1  = torch.tensor([tokenizer.encode(\"The planet earth\")]).to('cuda')\n",
    "\n",
    "loss = torch.ones_like(mymod(context1),device=device)\n",
    "tmp_local = {'self':mymod, 'src':context1, '_loss':loss}\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "max_before = torch.cuda.max_memory_allocated()\n",
    "allo_before = torch.cuda.memory_allocated()\n",
    "# checkmate schedule is translated into a list of code\n",
    "exec(GPT2_Sg.init_node.get_code(), globals(), tmp_local)\n",
    "\n",
    "for i, code in enumerate(sched_code):\n",
    "    exec(code, globals(), tmp_local)\n",
    "    print(f'code {i}', torch.cuda.memory_allocated()-allo_before, torch.cuda.max_memory_allocated()-max_before)\n",
    "    mem_timeline_real.append(torch.cuda.memory_allocated()-allo_before)\n",
    "    # allo_before = torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3614077a-73ce-453b-8bea-19e6b9b7d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4608 512\n",
      "___11__2 = torch.arange(0, __10__1, 1, dtype=torch.int32, device=torch.device('cuda'), pin_memory=False) ; __11__2 = ___11__2.detach()\n",
      "__13_input = torch.unsqueeze(__11__2, 0)\n",
      "1 4608 0\n",
      "\n",
      "3 4608 0\n",
      "\n",
      "4 13824 4608\n",
      "___18_fv = torch.embedding(self.wpe.weight, __13_input) ; __18_fv = ___18_fv.detach(); __18_fv.requires_grad_()\n",
      "5 512 0\n",
      "\n",
      "6 0 4608\n",
      "___20_fv = torch.dropout(torch.add(__16_fv, __18_fv), 0.0, True) ; __20_fv = ___20_fv.detach(); __20_fv.requires_grad_()\n",
      "7 4608 0\n",
      "\n",
      "8 512 5632\n",
      "___28_x = torch.layer_norm(__20_fv, [384], self.h[0].ln_1.weight, self.h[0].ln_1.bias) ; __28_x = ___28_x.detach(); __28_x.requires_grad_()\n",
      "__45__2 = torch.Tensor.size(__28_x, 1)\n",
      "__47__4 = torch.Tensor.size(__28_x, -1)\n",
      "__43__0 = torch.Tensor.size(__28_x, 0)\n",
      "__50_fv = torch.Tensor.view(__28_x, [-1, __47__4])\n",
      "9 18432 0\n",
      "\n",
      "10 4608 13824\n",
      "___48_x = torch.addmm(self.h[0].attn.c_attn.bias, __50_fv, self.h[0].attn.c_attn.weight) ; __48_x = ___48_x.detach(); __48_x.requires_grad_()\n",
      "__52_fv = torch.Tensor.view(__48_x, [__43__0, __45__2, 1152])\n",
      "__40__0 = torch.split(__52_fv, 384, 2)\n",
      "__56_x1 = __40__0[2]\n",
      "__81__17 = torch.Tensor.size(__56_x1, -1)\n",
      "__77__13 = torch.Tensor.size(__56_x1, 0)\n",
      "__79__15 = torch.Tensor.size(__56_x1, 1)\n",
      "__83_x4 = torch.Tensor.view(__56_x1, [__77__13, __79__15, 12, torch.div(__81__17, torch.tensor(12), rounding_mode='trunc')])\n",
      "__85_v = torch.permute(__83_x4, [0, 2, 1, 3])\n",
      "__54_x = __40__0[0]\n",
      "__61__5 = torch.Tensor.size(__54_x, -1)\n",
      "__57__1 = torch.Tensor.size(__54_x, 0)\n",
      "__59__3 = torch.Tensor.size(__54_x, 1)\n",
      "__63_x2 = torch.Tensor.view(__54_x, [__57__1, __59__3, 12, torch.div(__61__5, torch.tensor(12), rounding_mode='trunc')])\n",
      "__65_q = torch.permute(__63_x2, [0, 2, 1, 3])\n",
      "__55_x0 = __40__0[1]\n",
      "__67__7 = torch.Tensor.size(__55_x0, 0)\n",
      "__71__11 = torch.Tensor.size(__55_x0, -1)\n",
      "__69__9 = torch.Tensor.size(__55_x0, 1)\n",
      "__73_x3 = torch.Tensor.view(__55_x0, [__67__7, __69__9, 12, torch.div(__71__11, torch.tensor(12), rounding_mode='trunc')])\n",
      "__75_k = torch.permute(__73_x3, [0, 2, 1, 3])\n",
      "__88_fv = torch.transpose(__75_k, -2, -1)\n",
      "11 18432 0\n",
      "\n",
      "12 4608 512\n",
      "___87_scores = torch.matmul(__65_q, __88_fv) ; __87_scores = ___87_scores.detach(); __87_scores.requires_grad_()\n",
      "13 4608 0\n",
      "\n",
      "14 4608 512\n",
      "___90_fv = torch.softmax(__87_scores, -1) ; __90_fv = ___90_fv.detach(); __90_fv.requires_grad_()\n",
      "16 -4608 5632\n",
      "___33_x = torch.layer_norm(__20_fv, [384], self.h[0].ln_2.weight, self.h[0].ln_2.bias) ; __33_x = ___33_x.detach(); __33_x.requires_grad_()\n",
      "__126__2 = torch.Tensor.size(__33_x, 1)\n",
      "__124__0 = torch.Tensor.size(__33_x, 0)\n",
      "__128__4 = torch.Tensor.size(__33_x, -1)\n",
      "__131_fv = torch.Tensor.view(__33_x, [-1, __128__4])\n",
      "17 4608 0\n",
      "\n",
      "18 603136 0\n",
      "___91_fv = torch.dropout(__90_fv, 0.0, True) ; __91_fv = ___91_fv.detach(); __91_fv.requires_grad_()\n",
      "20 -603136 18432\n",
      "___129_x = torch.addmm(self.h[0].feedforward.c_fc.bias, __131_fv, self.h[0].feedforward.c_fc.weight) ; __129_x = ___129_x.detach(); __129_x.requires_grad_()\n",
      "__133_fv = torch.Tensor.view(__129_x, [__124__0, __126__2, 1536])\n",
      "21 4608 0\n",
      "\n",
      "22 -4608 4608\n",
      "___92_x5 = torch.matmul(__91_fv, __85_v) ; __92_x5 = ___92_x5.detach(); __92_x5.requires_grad_()\n",
      "__95_fv = torch.permute(__92_x5, [0, 2, 1, 3])\n",
      "23 13824 0\n",
      "\n",
      "24 -4608 18432\n",
      "___121_x = torch.nn.functional.gelu(__133_fv) ; __121_x = ___121_x.detach(); __121_x.requires_grad_()\n",
      "__139__2 = torch.Tensor.size(__121_x, 1)\n",
      "__141__4 = torch.Tensor.size(__121_x, -1)\n",
      "__137__0 = torch.Tensor.size(__121_x, 0)\n",
      "__144_fv = torch.Tensor.view(__121_x, [-1, __141__4])\n",
      "25 -4608 0\n",
      "\n",
      "27 -4608 0\n",
      "\n",
      "29 -4608 0\n",
      "\n",
      "30 -4608 4608\n",
      "___112_x0 = torch.addmm(self.h[0].attn.c_proj.bias, __114_fv, self.h[0].attn.c_proj.weight) ; __112_x0 = ___112_x0.detach(); __112_x0.requires_grad_()\n",
      "__116_fv = torch.Tensor.view(__112_x0, [__107__0, __109__2, 384])\n",
      "31 14336 0\n",
      "\n",
      "32 -512 0\n",
      "___147_fv = torch.dropout(__146_fv, 0.0, True) ; __147_fv = ___147_fv.detach(); __147_fv.requires_grad_()\n",
      "34 512 0\n",
      "\n",
      "35 4608 10240\n",
      "___151_input = torch.layer_norm(torch.add(torch.add(__20_fv, __116_fv), __147_fv), [384], self.ln_f.weight, self.ln_f.bias) ; __151_input = ___151_input.detach(); __151_input.requires_grad_()\n",
      "36 -4608 0\n",
      "\n",
      "37 18432 603136\n",
      "___153_fv = torch.nn.functional.linear(__151_input, self.wte.weight) ; __153_fv = ___153_fv.detach(); __153_fv.requires_grad_()\n",
      "38 -4608 0\n",
      "\n",
      "39 -18432 0\n",
      "__153_fv.grad = _loss ; loss = _loss.detach(); loss.requires_grad_()\n",
      "40 0 -603136\n",
      "__153_fv.data = torch.zeros(0,device=device); ___153_fv.data = torch.zeros(0,device=device);__153_fv.data = torch.zeros(0,device=device); \n",
      "41 512 0\n",
      "\n",
      "42 -512 77599232\n",
      "if ___153_fv.data.shape == torch.Size([0]):\n",
      "\t___153_fv.data = torch.zeros_like(__153_fv.grad,device=device)\n",
      "\t__153_fv.data = torch.zeros_like(__153_fv.grad,device=device)\n",
      "\t___153_fv.backward(__153_fv.grad)\n",
      "\t___153_fv.data = torch.zeros(0,device=device);\t__153_fv.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___153_fv.backward(__153_fv.grad)\n",
      "\n",
      "43 -512 -4608\n",
      "__151_input.data = torch.zeros(0,device=device); ___151_input.data = torch.zeros(0,device=device);__151_input.data = torch.zeros(0,device=device); \n",
      "44 18432 0\n",
      "\n",
      "45 -18432 11264\n",
      "if ___151_input.data.shape == torch.Size([0]):\n",
      "\t___151_input.data = torch.zeros_like(__151_input.grad,device=device)\n",
      "\t__151_input.data = torch.zeros_like(__151_input.grad,device=device)\n",
      "\t___151_input.backward(__151_input.grad)\n",
      "\t___151_input.data = torch.zeros(0,device=device);\t__151_input.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___151_input.backward(__151_input.grad)\n",
      "\n",
      "46 -18432 -4608\n",
      "__151_input.grad = None\n",
      "47 13824 -4608\n",
      "__112_x0.data = torch.zeros(0,device=device); ___112_x0.data = torch.zeros(0,device=device);__112_x0.data = torch.zeros(0,device=device); __116_fv.data = torch.zeros(0,device=device); \n",
      "48 -512 0\n",
      "\n",
      "49 -13824 595968\n",
      "if ___112_x0.data.shape == torch.Size([0]):\n",
      "\t___112_x0.data = torch.zeros_like(__112_x0.grad,device=device)\n",
      "\t__112_x0.data = torch.zeros_like(__112_x0.grad,device=device)\n",
      "\t___112_x0.backward(__112_x0.grad)\n",
      "\t___112_x0.data = torch.zeros(0,device=device);\t__112_x0.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___112_x0.backward(__112_x0.grad)\n",
      "\n",
      "50 4608 -4608\n",
      "__93_x6.data = torch.zeros(0,device=device); ___93_x6.data = torch.zeros(0,device=device);__93_x6.data = torch.zeros(0,device=device); __102_x7.data = torch.zeros(0,device=device); __114_fv.data = torch.zeros(0,device=device); \n",
      "51 4608 0\n",
      "\n",
      "53 4608 -4608\n",
      "__92_x5.data = torch.zeros(0,device=device); ___92_x5.data = torch.zeros(0,device=device);__92_x5.data = torch.zeros(0,device=device); __95_fv.data = torch.zeros(0,device=device); \n",
      "56 -4608 14336\n",
      "if ___92_x5.data.shape == torch.Size([0]):\n",
      "\t___92_x5.data = torch.zeros_like(__92_x5.grad,device=device)\n",
      "\t__92_x5.data = torch.zeros_like(__92_x5.grad,device=device)\n",
      "\t___92_x5.backward(__92_x5.grad)\n",
      "\t___92_x5.data = torch.zeros(0,device=device);\t__92_x5.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___92_x5.backward(__92_x5.grad)\n",
      "\n",
      "57 -14336 0\n",
      "__91_fv.data = torch.zeros(0,device=device); ___91_fv.data = torch.zeros(0,device=device);__91_fv.data = torch.zeros(0,device=device); \n",
      "58 -18432 -4608\n",
      "__92_x5.grad = None\n",
      "59 -13824 0\n",
      "\n",
      "60 4608 512\n",
      "if ___91_fv.data.shape == torch.Size([0]):\n",
      "\t___91_fv.data = torch.zeros_like(__91_fv.grad,device=device)\n",
      "\t__91_fv.data = torch.zeros_like(__91_fv.grad,device=device)\n",
      "\t___91_fv.backward(__91_fv.grad)\n",
      "\t___91_fv.data = torch.zeros(0,device=device);\t__91_fv.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___91_fv.backward(__91_fv.grad)\n",
      "\n",
      "61 -4608 0\n",
      "\n",
      "63 -4608 0\n",
      "__142_x0.data = torch.zeros(0,device=device); ___142_x0.data = torch.zeros(0,device=device);__142_x0.data = torch.zeros(0,device=device); __146_fv.data = torch.zeros(0,device=device); \n",
      "64 -4608 0\n",
      "\n",
      "65 9216 2379264\n",
      "if ___142_x0.data.shape == torch.Size([0]):\n",
      "\t___142_x0.data = torch.zeros_like(__142_x0.grad,device=device)\n",
      "\t__142_x0.data = torch.zeros_like(__142_x0.grad,device=device)\n",
      "\t___142_x0.backward(__142_x0.grad)\n",
      "\t___142_x0.data = torch.zeros(0,device=device);\t__142_x0.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___142_x0.backward(__142_x0.grad)\n",
      "\n",
      "66 -4608 0\n",
      "__142_x0.grad = None\n",
      "67 -4608 -18432\n",
      "__121_x.data = torch.zeros(0,device=device); ___121_x.data = torch.zeros(0,device=device);__121_x.data = torch.zeros(0,device=device); __144_fv.data = torch.zeros(0,device=device); \n",
      "68 -13824 -4608\n",
      "__147_fv.data = torch.zeros(0,device=device); ___147_fv.data = torch.zeros(0,device=device);__147_fv.data = torch.zeros(0,device=device); \n",
      "69 -4608 0\n",
      "\n",
      "70 -4608 512\n",
      "if ___90_fv.data.shape == torch.Size([0]):\n",
      "\t___90_fv.data = torch.zeros_like(__90_fv.grad,device=device)\n",
      "\t__90_fv.data = torch.zeros_like(__90_fv.grad,device=device)\n",
      "\t___90_fv.backward(__90_fv.grad)\n",
      "\t___90_fv.data = torch.zeros(0,device=device);\t__90_fv.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___90_fv.backward(__90_fv.grad)\n",
      "\n",
      "71 0 -512\n",
      "__90_fv.grad = None\n",
      "72 0 -512\n",
      "__87_scores.data = torch.zeros(0,device=device); ___87_scores.data = torch.zeros(0,device=device);__87_scores.data = torch.zeros(0,device=device); \n",
      "74 -512 18432\n",
      "if ___121_x.data.shape == torch.Size([0]):\n",
      "\t___121_x.data = torch.zeros_like(__121_x.grad,device=device)\n",
      "\t__121_x.data = torch.zeros_like(__121_x.grad,device=device)\n",
      "\t___121_x.backward(__121_x.grad)\n",
      "\t___121_x.data = torch.zeros(0,device=device);\t__121_x.data = torch.zeros(0,device=device)\n",
      "else:\n",
      "\t___121_x.backward(__121_x.grad)\n",
      "\n",
      "75 -9216 -18432\n",
      "__129_x.data = torch.zeros(0,device=device); ___129_x.data = torch.zeros(0,device=device);__129_x.data = torch.zeros(0,device=device); __133_fv.data = torch.zeros(0,device=device); \n",
      "76 0 -18432\n",
      "__121_x.grad = None\n"
     ]
    }
   ],
   "source": [
    "for i,_ in enumerate(mem_timeline[:-1]):\n",
    "    if mem_timeline[i+1] - mem_timeline[i] != mem_timeline_real[i+1] - mem_timeline_real[i]:\n",
    "        print(i,mem_timeline[i+1] - mem_timeline[i],mem_timeline_real[i+1] - mem_timeline_real[i])\n",
    "        print(sched_code[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a11b26-3457-4267-81b4-49b3d39ada6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "fig = figure(figsize=(8, 6), dpi=120)\n",
    "\n",
    "plt.plot(mem_timeline, label='theory')\n",
    "plt.plot(mem_timeline_real, label='practice')\n",
    "\n",
    "plt.xlabel('ops')\n",
    "plt.ylabel('mem')\n",
    "plt.title('Memory allocation')\n",
    "plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(\"mem_alloc.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2f48ad0-8407-4765-a05c-f9be0544cc42",
   "metadata": {},
   "source": [
    "for i,c in enumerate(sched_code):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0d197-0a14-498b-8f5c-6602b0b17a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
