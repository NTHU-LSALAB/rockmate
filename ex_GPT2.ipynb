{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8818dbb8-53d3-4417-94b4-fec607741ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules import ModuleList\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "import torch.nn as nn\n",
    "import pytorch_checkmate\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcb45e7c-1ab2-4735-bf00-533fbed48d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nx, nf):\n",
    "        super().__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(size_out)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dropout, d_model=768, nx=768*4):\n",
    "        super().__init__()\n",
    "        self.c_fc    = Conv1D(d_model, nx)\n",
    "        self.c_proj  = Conv1D(nx, d_model)\n",
    "        self.act     = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.act(self.c_fc(x))))\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_head  = n_head\n",
    "        self.d_model = d_model\n",
    "        self.c_attn  = Conv1D(d_model, d_model*3)\n",
    "        self.scale   = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.c_proj  = Conv1D(d_model, d_model)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        \"return shape [`batch`, `head`, `sequence`, `features`]\"\n",
    "        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head) \n",
    "        x = x.view(new_shape)\n",
    "        return x.permute(0, 2, 1, 3) \n",
    "    \n",
    "    def _attn(self, q, k, v, attn_mask=None):\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        outputs = torch.matmul(scores, v)\n",
    "        return outputs\n",
    "    \n",
    "    def merge_heads(self, x):\n",
    "        x         = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n",
    "        return x.view(new_shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n",
    "        q, k, v  = x.split(self.d_model, dim=2)\n",
    "        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
    "        # out      = self._attn(q, k, v)\n",
    "        scores  = torch.matmul(q, k.transpose(-2, -1))\n",
    "        # if self.scale: scores = scores/math.sqrt(v.size(-1))\n",
    "        nd, ns  = scores.size(-2), scores.size(-1)\n",
    "        # if attn_mask is not None: scores = scores + attn_mask\n",
    "        scores  = self.softmax(scores)\n",
    "        scores  = self.dropout(scores)\n",
    "        out = torch.matmul(scores, v)\n",
    "        \n",
    "        out      = self.merge_heads(out)\n",
    "        out      = self.c_proj(out)\n",
    "        return out\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn        = Attention(d_model=d_model, n_head=n_head, d_head=64, n_ctx=1024, bias=True, scale=False, dropout=dropout)\n",
    "        self.feedforward = FeedForward(dropout=dropout, d_model=d_model, nx=d_model*4)\n",
    "        self.ln_1        = LayerNorm(d_model)\n",
    "        self.ln_2        = LayerNorm(d_model)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = self.ln_1(x)\n",
    "        x2 = self.ln_2(x)\n",
    "        x = x + self.attn(x1)\n",
    "        x = x + self.feedforward(x2)\n",
    "        return x\n",
    "    \n",
    "def _get_clones(module, n):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(n)])\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257, dropout=0.1):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.nlayers = nlayers\n",
    "        block        = TransformerBlock(d_model=d_model, n_head=12, dropout=dropout)\n",
    "        self.h       = _get_clones(block, nlayers)\n",
    "        self.wte     = nn.Embedding(vcb_sz, d_model)\n",
    "        self.wpe     = nn.Embedding(n_ctx, d_model)\n",
    "        self.drop    = nn.Dropout(dropout)\n",
    "        self.ln_f    = LayerNorm(d_model)\n",
    "        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.out.weight = self.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, src, labels=None, pos_ids=None, return_inp=False, dropout=0.1):\n",
    "        if pos_ids is None: pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n",
    "        inp = self.drop((self.wte(src)+self.wpe(pos_ids)))\n",
    "        if return_inp: return inp \n",
    "        for i in range(self.nlayers): inp = self.h[0](inp)\n",
    "        inp     = self.ln_f(inp)\n",
    "        logits  = self.out(inp)\n",
    "        outputs = (logits,) + (inp,)\n",
    "        \n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "            return outputs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5ba95c-f90b-42f9-9df5-c7128eebd080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 464, 5440, 4534]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "context1  = torch.tensor([tokenizer.encode(\"The planet earth\")])\n",
    "context2  = torch.tensor([tokenizer.encode(\"I'm upset with those tools\")])\n",
    "print(context1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af07f6f-abc0-4e20-8143-f92ffa5477b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = GPT2(nlayers=1,dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289c36f5-cde5-41ac-92a7-34a9bfc3fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15862/4289802552.py:43: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n"
     ]
    }
   ],
   "source": [
    "GPT2_Bg = pytorch_checkmate.read_trace_code.make_B_graph(model2,(context1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae09c493-5848-4ed9-a5b5-d4f3306e284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch_checkmate.Dtools.print_all_fw_nodes(GPT2_Bg,print_ast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7fa401-6862-473a-a2f4-047cf2c2eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_Dg = pytorch_checkmate.Dtools.B_to_D(GPT2_Bg,model2,{\"src\":context1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ff81b6-ba04-4950-88b0-9d4124fae03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "pytorch_checkmate.Dtools.print_D_graph(GPT2_Dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "081b4ef3-3dd2-4479-ac6c-999ee6116d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_Sg = pytorch_checkmate.Stools.D_to_S(GPT2_Dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b092971-ae19-4104-8ec5-9c3a1aaf30e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n"
     ]
    }
   ],
   "source": [
    "pytorch_checkmate.Stools.print_S_graph(GPT2_Sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9766acc-2b55-4b85-b6f5-f5e784901709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n"
     ]
    }
   ],
   "source": [
    "GPT2_Kg = pytorch_checkmate.Ktools.S_to_K(GPT2_Sg,model2,{\"src\":context1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f32bf363-d677-48ab-80c8-df8813c1b47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "pytorch_checkmate.Ktools.print_K_graph(GPT2_Kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb925e5-f3d0-431f-8335-50f6f24564c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cost: 0 max cost: 0 budget: 100000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"gnome\" not found.\n",
      "Icon theme \"Mint-X\" not found.\n",
      "Icon theme \"elementary\" not found.\n",
      "Icon theme \"gnome\" not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feasible schedule solved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theotime/.local/lib/python3.9/site-packages/cvxpy/reductions/solvers/conic_solvers/scipy_conif.py:154: UserWarning: It is best to specify the 'method' parameter within scipy_options. The main advantage of this solver, is its ability to use the HiGHS LP solvers via scipy.optimize.linprog() which require a SciPy version >= 1.6.1 .\n",
      "\n",
      "The default method 'highs' will be used in this case.\n",
      "\n",
      "  warnings.warn(\"It is best to specify the 'method' parameter \"\n"
     ]
    }
   ],
   "source": [
    "budget = 100000000\n",
    "reload(pytorch_checkmate)\n",
    "sched_result, g = pytorch_checkmate.use_chk.make_sched(GPT2_Kg, budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd040d-8684-4ac2-bc65-b38811d3601c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
